{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-04T22:09:30.120182Z",
     "start_time": "2025-01-04T22:09:30.101674Z"
    }
   },
   "source": [
    "from pickletools import optimize\n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import PNDMScheduler, UNet2DConditionModel, AutoencoderKL\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPTokenizer, CLIPTextModel, CLIPImageProcessor"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T22:09:30.161752Z",
     "start_time": "2025-01-04T22:09:30.157759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_traced_unet(\n",
    "        checkpoint: str,\n",
    "        subfolder: str,\n",
    "        filename: str,\n",
    "        dtype: torch.dtype,\n",
    "        device: str = \"cuda\",\n",
    "        local_files_only=False,\n",
    "        cache_dir: Optional[str] = None,\n",
    ") -> Optional[nn.Module]:\n",
    "    unet_file = hf_hub_download(\n",
    "        checkpoint,\n",
    "        subfolder=subfolder,\n",
    "        filename=filename,\n",
    "        local_files_only=local_files_only,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    unet_traced = torch.jit.load(unet_file)\n",
    "\n",
    "    class TracedUNet(nn.Module):\n",
    "        @dataclass\n",
    "        class UNet2DConditionOutput:\n",
    "            sample: torch.Tensor\n",
    "\n",
    "        def __init__(self):\n",
    "            super(TracedUNet, self).__init__()\n",
    "            self.in_channels = device\n",
    "            self.device = device\n",
    "            self.dtype = dtype\n",
    "\n",
    "        def forward(self, latent_model_input, t, encoder_hidden_states):\n",
    "            sample = unet_traced(latent_model_input, t, encoder_hidden_states)[0]\n",
    "            return self.UNet2DConditionOutput(sample=sample)\n",
    "\n",
    "    return TracedUNet()"
   ],
   "id": "d681ffd606c9f90b",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T22:09:30.212051Z",
     "start_time": "2025-01-04T22:09:30.207737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TVE(nn.Module):\n",
    "    def __init__(self, embed_dim=768, ff_dim=1024, att_heads=8, cross_att_heads=8):\n",
    "        super(TVE, self).__init__()\n",
    "        self.embed = nn.Linear(1, embed_dim)\n",
    "        self.att = nn.MultiheadAttention(embed_dim, att_heads)\n",
    "        self.cross_att = nn.MultiheadAttention(embed_dim, cross_att_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "                nn.Linear(embed_dim, ff_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, timestep, text_embed):\n",
    "        t_e = self.embed(timestep)\n",
    "        v_0 = t_e + text_embed.to(t_e.dtype)\n",
    "        v_1, _ = self.att(v_0, v_0, v_0)\n",
    "        v_2, _ = self.cross_att(v_1, v_0, v_0)\n",
    "        v_i = self.ff(v_2)\n",
    "        return v_i\n",
    "\n",
    "class TextTransform(nn.Module):\n",
    "    def __init__(self, MODEL=\"riffusion/riffusion-model-v1\"):\n",
    "        super(TextTransform, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(MODEL, subfolder=\"text_encoder\")\n",
    "        self.text_encoder = self.text_encoder.to(self.device)\n",
    "\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(MODEL, subfolder=\"tokenizer\")\n",
    "        self.tve = TVE().to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_text(self, prompt):\n",
    "        text_inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "\n",
    "        prompt_embeds = self.text_encoder(text_input_ids.to(self.device))\n",
    "        prompt_embeds = prompt_embeds[0]\n",
    "        prompt_embeds_dtype = self.text_encoder.dtype\n",
    "        prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=self.device)\n",
    "\n",
    "        return prompt_embeds\n",
    "\n",
    "    def forward(self, timestep, text):\n",
    "        text_embed = self.embed_text(text)\n",
    "        return self.tve(timestep, text_embed)"
   ],
   "id": "c690d7f4d72511db",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T22:09:30.267710Z",
     "start_time": "2025-01-04T22:09:30.258005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SoundStyleTransferModel(nn.Module):\n",
    "    def __init__(self, MODEL=\"riffusion/riffusion-model-v1\"):\n",
    "        super(SoundStyleTransferModel, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.frozen_clip = TextTransform()\n",
    "\n",
    "        self.vae = AutoencoderKL.from_pretrained(MODEL, subfolder=\"vae\")\n",
    "        self.vae = self.vae.to(self.device)\n",
    "\n",
    "        self.unet = UNet2DConditionModel.from_pretrained(MODEL, subfolder=\"unet\")\n",
    "        self.unet = self.unet.to(self.device)\n",
    "\n",
    "        self.scheduler = PNDMScheduler.from_config(MODEL, subfolder=\"scheduler\")\n",
    "        self.scheduler.prk_timesteps = np.array([])\n",
    "\n",
    "        traced_unet = load_traced_unet(\n",
    "            MODEL,\n",
    "            subfolder=\"unet_traced\",\n",
    "            filename=\"unet_traced.pt\",\n",
    "            dtype=torch.float32,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        if traced_unet is not None:\n",
    "            print(\"Loaded Traced UNet\")\n",
    "            self.unet = traced_unet\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_images(self, images):\n",
    "        return self.vae.encode(images).latent_dist.sample() * 0.18215\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode_latents(self, latents):\n",
    "        return self.vae.decode(latents / 0.18215).sample\n",
    "\n",
    "    def forward(self, latents, text_embeddings, timesteps):\n",
    "        result = self.unet.forward(\n",
    "            latents,\n",
    "            timesteps,\n",
    "            text_embeddings\n",
    "        ).sample\n",
    "        return result\n",
    "\n",
    "    def get_text_embeddings(self, alpha, text_prompt_start, text_prompt_end):\n",
    "        embed_start = self.frozen_clip.embed_text(text_prompt_start)\n",
    "        embed_end = self.frozen_clip.embed_text(text_prompt_end)\n",
    "        text_embed = embed_start * (1.0 - alpha) + embed_end * alpha\n",
    "        return text_embed, embed_start.dtype\n",
    "\n",
    "    def original_timestep(self, alpha, denoising_a, denoising_b, inference_steps):\n",
    "        strength = (1 - alpha) * denoising_a + alpha * denoising_b\n",
    "\n",
    "        offset = self.scheduler.config.get(\"steps_offset\", 0)\n",
    "        init_timestep = int(inference_steps * strength) + offset\n",
    "        init_timestep = min(init_timestep, inference_steps)\n",
    "\n",
    "        timesteps = self.scheduler.timesteps[-init_timestep]\n",
    "        timesteps = torch.tensor([timesteps], device=self.device)\n",
    "\n",
    "        return timesteps, init_timestep, offset\n",
    "\n",
    "    def partial_diffusion(self, latents, alpha, timesteps, dtype):\n",
    "        noise_a = torch.randn(latents.shape, device=self.device, dtype=dtype)\n",
    "        noise_b = torch.randn(latents.shape, device=self.device, dtype=dtype)\n",
    "        noise = self.slerp(alpha, noise_a, noise_b)\n",
    "        latents = self.scheduler.add_noise(latents, noise, timesteps)\n",
    "        return latents\n",
    "\n",
    "    def get_extra_kwargs(self, eta):\n",
    "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
    "        extra_step_kwargs = {}\n",
    "        if accepts_eta:\n",
    "            extra_step_kwargs[\"eta\"] = eta\n",
    "        return extra_step_kwargs\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def diffuse(\n",
    "            self,\n",
    "            init_image,\n",
    "            text_prompt_start,\n",
    "            text_prompt_end,\n",
    "            inference_steps=50,\n",
    "            denoising_a=0.75,\n",
    "            denoising_b=0.75,\n",
    "            guidance_a=7.0,\n",
    "            guidance_b=7.0,\n",
    "            alpha=0.75,\n",
    "            eta=0.00,\n",
    "            num_images_per_prompt=1,\n",
    "    ):\n",
    "        self.unet.eval()\n",
    "        self.scheduler.set_timesteps(inference_steps)\n",
    "\n",
    "        # Guidance for later\n",
    "        guidance_scale = guidance_a * (1.0 - alpha) + guidance_b * alpha\n",
    "\n",
    "        text_embed, latents_dtype = self.get_text_embeddings(alpha, text_prompt_start, text_prompt_end)\n",
    "\n",
    "        # Duplicate text embeddingsbs_embed, seq_len, _ = text_embeddings.shape\n",
    "        bs_embed, seq_len, _ = text_embed.shape\n",
    "        text_embed = text_embed.repeat(1, num_images_per_prompt, 1)\n",
    "        text_embed = text_embed.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        # Negative prompt for guidance\n",
    "        if guidance_scale > 1:\n",
    "            uncond_tokens = [\"\"]\n",
    "            uncond_input = self.frozen_clip.tokenizer(\n",
    "                uncond_tokens,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.frozen_clip.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            uncond_ids = uncond_input.input_ids.to(self.device)\n",
    "            uncond_embed = self.frozen_clip.text_encoder(uncond_ids)[0]\n",
    "            uncond_embed = uncond_embed.repeat_interleave(bs_embed * num_images_per_prompt, dim=0)\n",
    "\n",
    "            text_embed = torch.cat([uncond_embed, text_embed])\n",
    "\n",
    "        image_torch = self.preprocess_image(init_image).to(device=self.device, dtype=latents_dtype)\n",
    "        init_latents = self.encode_images(image_torch)\n",
    "        print(init_latents.shape)\n",
    "\n",
    "        # Partial diffusion\n",
    "        timesteps, init_timestep, offset = self.original_timestep(alpha, denoising_a, denoising_b, inference_steps)\n",
    "        init_latents = self.partial_diffusion(init_latents, alpha, timesteps, dtype=latents_dtype)\n",
    "\n",
    "        extra_step_kwargs = self.get_extra_kwargs(eta)\n",
    "        t_start = max(inference_steps - init_timestep + offset, 0)\n",
    "        timesteps = self.scheduler.timesteps[t_start:].to(self.device)\n",
    "\n",
    "        latents = init_latents\n",
    "        for t in tqdm(timesteps, total=len(timesteps)):\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                latent_input = torch.cat([latents, latents]) if guidance_scale > 1 else latents\n",
    "                latent_input = self.scheduler.scale_model_input(latent_input, t)\n",
    "\n",
    "                pred_noise = self.unet(latent_input, t, text_embed).sample\n",
    "\n",
    "                if guidance_scale > 1:\n",
    "                    pred_noise_uncond, pred_noise_text = pred_noise.chunk(2)\n",
    "                    pred_noise = pred_noise_uncond + guidance_scale * (pred_noise_text - pred_noise_uncond)\n",
    "\n",
    "                latents = self.scheduler.step(pred_noise, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "        decoded_image = self.decode_latents(latents)\n",
    "        image = (decoded_image / 2 + 0.5).clamp(0, 1).cpu().permute(0, 2, 3, 1).squeeze().numpy()\n",
    "        image = self.numpy_to_pil(image)[0]\n",
    "\n",
    "        return image\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_image(image: Image.Image) -> torch.Tensor:\n",
    "        w, h = image.size\n",
    "        transformer = transforms.Compose([\n",
    "            transforms.Resize((w - w % 32, h - h % 32), interpolation=Image.LANCZOS),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        image_torch = transformer(image).permute(0, 2, 1).unsqueeze(0)\n",
    "        return 2.0 * image_torch - 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def slerp(t: float, v0: torch.Tensor, v1: torch.Tensor, dot_threshold: float = 0.9995) -> torch.Tensor:\n",
    "        if not isinstance(v0, np.ndarray):\n",
    "            inputs_are_torch = True\n",
    "            input_device = v0.device\n",
    "            v0 = v0.cpu().numpy()\n",
    "            v1 = v1.cpu().numpy()\n",
    "\n",
    "        dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))\n",
    "        if np.abs(dot) > dot_threshold:\n",
    "            v2 = (1 - t) * v0 + t * v1\n",
    "        else:\n",
    "            theta_0 = np.arccos(dot)\n",
    "            sin_theta_0 = np.sin(theta_0)\n",
    "            theta_t = theta_0 * t\n",
    "            sin_theta_t = np.sin(theta_t)\n",
    "            s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
    "            s1 = sin_theta_t / sin_theta_0\n",
    "            v2 = s0 * v0 + s1 * v1\n",
    "\n",
    "        if inputs_are_torch:\n",
    "            v2 = torch.from_numpy(v2).to(input_device)\n",
    "\n",
    "        return v2\n",
    "\n",
    "    @staticmethod\n",
    "    def numpy_to_pil(images):\n",
    "        if images.ndim == 3:\n",
    "            images = images[None, ...]\n",
    "        images = (images * 255).round().astype(\"uint8\")\n",
    "        if images.shape[-1] == 1:\n",
    "            pil_images = [Image.fromarray(image.squeeze(), mode=\"L\") for image in images]\n",
    "        else:\n",
    "            pil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "        return pil_images"
   ],
   "id": "554cc8f02149c6b6",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T22:09:36.239478Z",
     "start_time": "2025-01-04T22:09:30.320704Z"
    }
   },
   "cell_type": "code",
   "source": "model = SoundStyleTransferModel()",
   "id": "dac3e2b0528fe20c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch riffusion/riffusion-model-v1: riffusion/riffusion-model-v1 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch riffusion/riffusion-model-v1: riffusion/riffusion-model-v1 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Traced UNet\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T22:09:36.258160Z",
     "start_time": "2025-01-04T22:09:36.255642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# image = Image.open(\"sample.png\")\n",
    "# print(image.size)\n",
    "#\n",
    "# prompt_start = \"church bells on sunday\"\n",
    "# prompt_end = \"jazz with piano\"\n",
    "#\n",
    "# image = model.diffuse(image, prompt_start, prompt_end, alpha=0.8)\n",
    "#\n",
    "# image.save(\"out_sample.png\")\n",
    "# print(image.size)"
   ],
   "id": "d900bfc2c63eef0f",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train TVEa",
   "id": "4ff2de6765d35793"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T22:11:16.145625Z",
     "start_time": "2025-01-04T22:11:15.664052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 1\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.frozen_clip.tve.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    model.frozen_clip.tve.train()\n",
    "    for step, (images, labels) in enumerate([(1, \"tekst\")]):\n",
    "        images, labels = images, labels\n",
    "\n",
    "        #batch_size = images.size(0)\n",
    "        batch_size = 1\n",
    "\n",
    "        init_latents = model.encode_images(images)\n",
    "        noise = torch.randn_like(init_latents)\n",
    "\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            model.scheduler.config.num_train_timesteps,\n",
    "            (batch_size,),\n",
    "            dtype=torch.int64\n",
    "        )\n",
    "\n",
    "        label_embeddings = [model.frozen_clip(timesteps, label) for label in labels]\n",
    "        label_embeddings = torch.stack(label_embeddings).squeeze(dim=1).to(device=model.device)\n",
    "\n",
    "        # Predict noise\n",
    "        pred_noise = model(init_latents, label_embeddings, timesteps)\n",
    "        loss = criterion(pred_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item()}\")"
   ],
   "id": "f286438e7c5714fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 768])\n",
      "torch.Size([5, 77, 768])\n",
      "torch.Size([1, 77, 768])\n",
      "torch.Size([5, 77, 768])\n",
      "torch.Size([1, 77, 768])\n",
      "torch.Size([5, 77, 768])\n",
      "torch.Size([1, 77, 768])\n",
      "torch.Size([5, 77, 768])\n",
      "torch.Size([1, 77, 768])\n",
      "torch.Size([5, 77, 768])\n",
      "torch.Size([1, 77, 768])\n",
      "torch.Size([5, 77, 768])\n",
      "torch.Size([1, 77, 768])\n",
      "torch.Size([5, 77, 768])\n",
      "torch.Size([1, 77, 768])\n",
      "torch.Size([5, 77, 768])\n",
      "torch.Size([1, 77, 768])\n",
      "torch.Size([5, 77, 768])\n",
      "torch.Size([1, 77, 768])\n",
      "torch.Size([5, 77, 768])\n"
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
