{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T22:33:12.695774Z",
     "start_time": "2025-01-12T22:33:12.652070Z"
    }
   },
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.style_transfer import SoundStyleTransferModel"
   ],
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T22:33:19.270968Z",
     "start_time": "2025-01-12T22:33:12.699372Z"
    }
   },
   "cell_type": "code",
   "source": "model = SoundStyleTransferModel()",
   "id": "dac3e2b0528fe20c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch riffusion/riffusion-model-v1: riffusion/riffusion-model-v1 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch riffusion/riffusion-model-v1: riffusion/riffusion-model-v1 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Traced UNet\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train TVE",
   "id": "4ff2de6765d35793"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T22:33:19.321346Z",
     "start_time": "2025-01-12T22:33:19.282479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_audio_dataloader(batch_size=1, dataset=\"church_bell\"):\n",
    "    images = []\n",
    "    path = f\"./audios/timbre/{dataset}\"\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            image = Image.open(os.path.join(path, filename))\n",
    "            image = SoundStyleTransferModel.preprocess_image(image)\n",
    "            image = image.squeeze(0)\n",
    "            images.append(image)\n",
    "\n",
    "    class MyDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, dataset, label):\n",
    "            self.dataset = dataset\n",
    "            self.label = label\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.dataset[idx], self.label\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(MyDataset(images, dataset), batch_size=batch_size, shuffle=True)\n",
    "    return dataloader"
   ],
   "id": "c330fb22d9569bb3",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T22:33:19.383034Z",
     "start_time": "2025-01-12T22:33:19.345992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# learning_rate = 0.001\n",
    "# batch_size = 1\n",
    "#\n",
    "# for param in model.text_transform.text_encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "#\n",
    "# for param in model.unet.parameters():\n",
    "#     param.requires_grad = False\n",
    "#\n",
    "# optimizer = torch.optim.AdamW(model.text_transform.tve.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "# criterion = nn.MSELoss()\n",
    "# scaler = torch.amp.GradScaler(\"cuda\")\n",
    "#\n",
    "# dataset = \"cornet\"\n",
    "# dataloader = get_audio_dataloader(batch_size=batch_size, dataset=dataset)\n",
    "#\n",
    "# num_epochs = 2000\n",
    "# tqdm_bar = tqdm(range(num_epochs))\n",
    "# model.text_transform.tve.train()\n",
    "# for epoch in tqdm_bar:\n",
    "#     epoch_loss = 0\n",
    "#     for step, (images, labels) in enumerate(dataloader):\n",
    "#         images, labels = images.to(device=model.device), [\"*\"] * batch_size\n",
    "#\n",
    "#         with torch.no_grad():\n",
    "#             init_latents = model.encode_images(images)\n",
    "#\n",
    "#         noise = torch.randn_like(init_latents)\n",
    "#         timesteps = torch.randint(\n",
    "#             0,\n",
    "#             model.scheduler.config.num_train_timesteps,\n",
    "#             (batch_size,),\n",
    "#             dtype=torch.int64,\n",
    "#             device=model.device\n",
    "#         )\n",
    "#         noisy_latents = model.scheduler.add_noise(init_latents, noise, timesteps)\n",
    "#\n",
    "#         with torch.no_grad():\n",
    "#             text_embeddings = [model.text_transform.embed_text(label) for label in labels]\n",
    "#             text_embeddings = torch.stack(text_embeddings).squeeze(dim=1).to(device=model.device)\n",
    "#\n",
    "#         label_embeddings = model.text_transform.tve(timesteps, text_embeddings)\n",
    "#\n",
    "#         with torch.amp.autocast(\"cuda\"):\n",
    "#             pred_noise = model(noisy_latents, label_embeddings, timesteps)\n",
    "#             loss = criterion(noise, pred_noise)\n",
    "#             epoch_loss += loss.item()\n",
    "#\n",
    "#         optimizer.zero_grad()\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#\n",
    "#     tqdm_bar.set_description(f\"Loss: {loss.item() / len(dataloader)}\")\n",
    "#\n",
    "# torch.save(model.text_transform.tve.state_dict(), \"./data/tve.pth\")"
   ],
   "id": "f286438e7c5714fe",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sample sound",
   "id": "eda5605318963bac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T22:34:36.557834Z",
     "start_time": "2025-01-12T22:34:35.547700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.text_transform.tve.load_state_dict(torch.load(\"./data/tve.pth\", weights_only=True))\n",
    "image = Image.open(\"sample.png\")\n",
    "print(image.size)\n",
    "#image = Image.open(\"./audios/timbre/cornet/data1.png\")\n",
    "\n",
    "prompt = \"*\"\n",
    "image = model.pipe_paper(\n",
    "    image,\n",
    "    prompt,\n",
    "    inference_steps=50,\n",
    "    scale=3.5,\n",
    "    strength=1,\n",
    "    alpha=0.5,\n",
    "    tve=True\n",
    ")\n",
    "#image = model.transfer_style(image, \"cornet\", \"church bell\")\n",
    "\n",
    "image.save(\"out_sample.png\")\n",
    "print(image.size)"
   ],
   "id": "e272c075f125f934",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(502, 512)\n",
      "cuda\n",
      "torch.Size([1, 3, 480, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/diffusers/models/unet_2d_condition/___torch_mangle_4939.py\", line 53, in forward\n    _19 = (mid_block).forward(_17, _5, input, )\n    _21 = (_0).forward(_19, _17, _5, _18, _14, )\n    _22 = (_1).forward(_21, _15, _5, input, _16, _11, )\n           ~~~~~~~~~~~ <--- HERE\n    _23 = (_2).forward(_22, _12, _5, input, _13, _7, )\n    _24 = (_3).forward(_23, _8, _5, input, _9, _6, )\n  File \"code/__torch__/diffusers/models/unet_2d_blocks/___torch_mangle_4649.py\", line 30, in forward\n    resnets1 = self.resnets\n    _01 = getattr(resnets1, \"0\")\n    input0 = torch.cat([argument_1, argument_2], 1)\n             ~~~~~~~~~ <--- HERE\n    _3 = (_00).forward((_01).forward(input0, argument_3, ), input, )\n    input1 = torch.cat([_3, argument_5], 1)\n\nTraceback of TorchScript, original code (most recent call last):\n/home/ubuntu/.local/lib/python3.8/site-packages/diffusers/models/unet_2d_blocks.py(1184): forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1190): _call_impl\n/home/ubuntu/.local/lib/python3.8/site-packages/diffusers/models/unet_2d_condition.py(407): forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1190): _call_impl\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/jit/_trace.py(976): trace_module\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/jit/_trace.py(759): trace\n/home/ubuntu/riffusion-model-truss/model/tracingScript.py(72): <module>\n/usr/lib/python3/dist-packages/IPython/utils/py3compat.py(168): execfile\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(2720): safe_execfile\n/usr/lib/python3/dist-packages/IPython/core/magics/execution.py(812): run\n/usr/lib/python3/dist-packages/IPython/core/magics/execution.py(827): run\n/usr/lib/python3/dist-packages/IPython/core/magic.py(187): <lambda>\n<decorator-gen-52>(2): run\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(2317): run_line_magic\n<ipython-input-42-d0bdce5bda7e>(1): <module>\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(3331): run_code\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(3254): run_ast_nodes\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(3062): run_cell_async\n/usr/lib/python3/dist-packages/IPython/core/async_helpers.py(68): _pseudo_sync_runner\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(2886): _run_cell\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(2857): run_cell\n/usr/lib/python3/dist-packages/ipykernel/zmqshell.py(536): run_cell\n/usr/lib/python3/dist-packages/ipykernel/ipkernel.py(300): do_execute\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/gen.py(234): wrapper\n/usr/lib/python3/dist-packages/ipykernel/kernelbase.py(543): execute_request\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/gen.py(234): wrapper\n/usr/lib/python3/dist-packages/ipykernel/kernelbase.py(268): dispatch_shell\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/gen.py(234): wrapper\n/usr/lib/python3/dist-packages/ipykernel/kernelbase.py(365): process_one\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/gen.py(782): run\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/gen.py(821): inner\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/ioloop.py(740): _run_callback\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/ioloop.py(687): <lambda>\n/usr/lib/python3.8/asyncio/events.py(81): _run\n/usr/lib/python3.8/asyncio/base_events.py(1859): _run_once\n/usr/lib/python3.8/asyncio/base_events.py(570): run_forever\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py(215): start\n/usr/lib/python3/dist-packages/ipykernel/kernelapp.py(583): start\n/home/ubuntu/.local/lib/python3.8/site-packages/traitlets/config/application.py(982): launch_instance\n/usr/lib/python3/dist-packages/ipykernel_launcher.py(16): <module>\n/usr/lib/python3.8/runpy.py(87): _run_code\n/usr/lib/python3.8/runpy.py(194): _run_module_as_main\nRuntimeError: Sizes of tensors must match except in dimension 1. Expected size 16 but got size 15 for tensor number 1 in the list.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[118], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#image = Image.open(\"./audios/timbre/cornet/data1.png\")\u001B[39;00m\n\u001B[1;32m      6\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 7\u001B[0m image \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpipe_paper(\n\u001B[1;32m      8\u001B[0m     image,\n\u001B[1;32m      9\u001B[0m     prompt,\n\u001B[1;32m     10\u001B[0m     inference_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m,\n\u001B[1;32m     11\u001B[0m     scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3.5\u001B[39m,\n\u001B[1;32m     12\u001B[0m     strength\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     13\u001B[0m     alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m,\n\u001B[1;32m     14\u001B[0m     tve\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     15\u001B[0m )\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m#image = model.transfer_style(image, \"cornet\", \"church bell\")\u001B[39;00m\n\u001B[1;32m     18\u001B[0m image\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mout_sample.png\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/sound-transfer/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/dev/sound-style-transfer-diffusion/models/style_transfer.py:338\u001B[0m, in \u001B[0;36mSoundStyleTransferModel.pipe_paper\u001B[0;34m(self, mel_spectrogram, text_prompt, inference_steps, strength, scale, alpha, tve)\u001B[0m\n\u001B[1;32m    335\u001B[0m latent_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler\u001B[38;5;241m.\u001B[39mscale_model_input(latent_input, t)\n\u001B[1;32m    337\u001B[0m text_embed, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_text_embed_test(text_prompt, scale, t, tve\u001B[38;5;241m=\u001B[39mtve)\n\u001B[0;32m--> 338\u001B[0m pred_noise \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munet(latent_input, t, text_embed)\u001B[38;5;241m.\u001B[39msample\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m scale \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    341\u001B[0m     pred_noise_uncond, pred_noise_text \u001B[38;5;241m=\u001B[39m pred_noise\u001B[38;5;241m.\u001B[39mchunk(\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/sound-transfer/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/sound-transfer/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/dev/sound-style-transfer-diffusion/models/style_transfer.py:47\u001B[0m, in \u001B[0;36mload_traced_unet.<locals>.TracedUNet.forward\u001B[0;34m(self, latent_model_input, t, encoder_hidden_states)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, latent_model_input, t, encoder_hidden_states):\n\u001B[0;32m---> 47\u001B[0m     sample \u001B[38;5;241m=\u001B[39m unet_traced(latent_model_input, t, encoder_hidden_states)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mUNet2DConditionOutput(sample\u001B[38;5;241m=\u001B[39msample)\n",
      "File \u001B[0;32m~/miniconda3/envs/sound-transfer/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/sound-transfer/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/diffusers/models/unet_2d_condition/___torch_mangle_4939.py\", line 53, in forward\n    _19 = (mid_block).forward(_17, _5, input, )\n    _21 = (_0).forward(_19, _17, _5, _18, _14, )\n    _22 = (_1).forward(_21, _15, _5, input, _16, _11, )\n           ~~~~~~~~~~~ <--- HERE\n    _23 = (_2).forward(_22, _12, _5, input, _13, _7, )\n    _24 = (_3).forward(_23, _8, _5, input, _9, _6, )\n  File \"code/__torch__/diffusers/models/unet_2d_blocks/___torch_mangle_4649.py\", line 30, in forward\n    resnets1 = self.resnets\n    _01 = getattr(resnets1, \"0\")\n    input0 = torch.cat([argument_1, argument_2], 1)\n             ~~~~~~~~~ <--- HERE\n    _3 = (_00).forward((_01).forward(input0, argument_3, ), input, )\n    input1 = torch.cat([_3, argument_5], 1)\n\nTraceback of TorchScript, original code (most recent call last):\n/home/ubuntu/.local/lib/python3.8/site-packages/diffusers/models/unet_2d_blocks.py(1184): forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1190): _call_impl\n/home/ubuntu/.local/lib/python3.8/site-packages/diffusers/models/unet_2d_condition.py(407): forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1190): _call_impl\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/jit/_trace.py(976): trace_module\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/jit/_trace.py(759): trace\n/home/ubuntu/riffusion-model-truss/model/tracingScript.py(72): <module>\n/usr/lib/python3/dist-packages/IPython/utils/py3compat.py(168): execfile\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(2720): safe_execfile\n/usr/lib/python3/dist-packages/IPython/core/magics/execution.py(812): run\n/usr/lib/python3/dist-packages/IPython/core/magics/execution.py(827): run\n/usr/lib/python3/dist-packages/IPython/core/magic.py(187): <lambda>\n<decorator-gen-52>(2): run\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(2317): run_line_magic\n<ipython-input-42-d0bdce5bda7e>(1): <module>\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(3331): run_code\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(3254): run_ast_nodes\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(3062): run_cell_async\n/usr/lib/python3/dist-packages/IPython/core/async_helpers.py(68): _pseudo_sync_runner\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(2886): _run_cell\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py(2857): run_cell\n/usr/lib/python3/dist-packages/ipykernel/zmqshell.py(536): run_cell\n/usr/lib/python3/dist-packages/ipykernel/ipkernel.py(300): do_execute\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/gen.py(234): wrapper\n/usr/lib/python3/dist-packages/ipykernel/kernelbase.py(543): execute_request\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/gen.py(234): wrapper\n/usr/lib/python3/dist-packages/ipykernel/kernelbase.py(268): dispatch_shell\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/gen.py(234): wrapper\n/usr/lib/python3/dist-packages/ipykernel/kernelbase.py(365): process_one\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/gen.py(782): run\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/gen.py(821): inner\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/ioloop.py(740): _run_callback\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/ioloop.py(687): <lambda>\n/usr/lib/python3.8/asyncio/events.py(81): _run\n/usr/lib/python3.8/asyncio/base_events.py(1859): _run_once\n/usr/lib/python3.8/asyncio/base_events.py(570): run_forever\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py(215): start\n/usr/lib/python3/dist-packages/ipykernel/kernelapp.py(583): start\n/home/ubuntu/.local/lib/python3.8/site-packages/traitlets/config/application.py(982): launch_instance\n/usr/lib/python3/dist-packages/ipykernel_launcher.py(16): <module>\n/usr/lib/python3.8/runpy.py(87): _run_code\n/usr/lib/python3.8/runpy.py(194): _run_module_as_main\nRuntimeError: Sizes of tensors must match except in dimension 1. Expected size 16 but got size 15 for tensor number 1 in the list.\n"
     ]
    }
   ],
   "execution_count": 118
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
