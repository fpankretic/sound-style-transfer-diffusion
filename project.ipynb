{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-04T17:53:44.894808Z",
     "start_time": "2025-01-04T17:53:44.862141Z"
    }
   },
   "source": [
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import PNDMScheduler, UNet2DConditionModel, AutoencoderKL\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPTokenizer, CLIPTextModel, CLIPImageProcessor"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T17:53:44.901271Z",
     "start_time": "2025-01-04T17:53:44.897574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_traced_unet(\n",
    "        checkpoint: str,\n",
    "        subfolder: str,\n",
    "        filename: str,\n",
    "        dtype: torch.dtype,\n",
    "        device: str = \"cuda\",\n",
    "        local_files_only=False,\n",
    "        cache_dir: Optional[str] = None,\n",
    ") -> Optional[nn.Module]:\n",
    "    unet_file = hf_hub_download(\n",
    "        checkpoint,\n",
    "        subfolder=subfolder,\n",
    "        filename=filename,\n",
    "        local_files_only=local_files_only,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    unet_traced = torch.jit.load(unet_file)\n",
    "\n",
    "    class TracedUNet(nn.Module):\n",
    "        @dataclass\n",
    "        class UNet2DConditionOutput:\n",
    "            sample: torch.Tensor\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.in_channels = device\n",
    "            self.device = device\n",
    "            self.dtype = dtype\n",
    "\n",
    "        def forward(self, latent_model_input, t, encoder_hidden_states):\n",
    "            sample = unet_traced(latent_model_input, t, encoder_hidden_states)[0]\n",
    "            return self.UNet2DConditionOutput(sample=sample)\n",
    "\n",
    "    return TracedUNet()"
   ],
   "id": "d681ffd606c9f90b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T17:53:44.953992Z",
     "start_time": "2025-01-04T17:53:44.950124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FrozenCLIP(nn.Module):\n",
    "    def __init__(self, MODEL=\"riffusion/riffusion-model-v1\"):\n",
    "        super(FrozenCLIP, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(MODEL, subfolder=\"text_encoder\")\n",
    "        self.text_encoder = self.text_encoder.to(self.device)\n",
    "\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(MODEL, subfolder=\"tokenizer\")\n",
    "        self.feature_extractor = CLIPImageProcessor.from_pretrained(MODEL, subfolder=\"feature_extractor\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_text(self, prompt):\n",
    "        text_inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "\n",
    "        prompt_embeds = self.text_encoder(text_input_ids.to(self.device))\n",
    "        prompt_embeds = prompt_embeds[0]\n",
    "        prompt_embeds_dtype = self.text_encoder.dtype\n",
    "        prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=self.device)\n",
    "\n",
    "        return prompt_embeds"
   ],
   "id": "c690d7f4d72511db",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T17:53:45.008574Z",
     "start_time": "2025-01-04T17:53:45.000381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SoundStyleTransferModel(nn.Module):\n",
    "    def __init__(self, MODEL=\"riffusion/riffusion-model-v1\"):\n",
    "        super(SoundStyleTransferModel, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.frozen_clip = FrozenCLIP()\n",
    "\n",
    "        self.vae = AutoencoderKL.from_pretrained(MODEL, subfolder=\"vae\")\n",
    "        self.vae = self.vae.to(self.device)\n",
    "\n",
    "        self.unet = UNet2DConditionModel.from_pretrained(MODEL, subfolder=\"unet\")\n",
    "        self.unet = self.unet.to(self.device)\n",
    "\n",
    "        self.scheduler = PNDMScheduler.from_config(MODEL, subfolder=\"scheduler\")\n",
    "        self.scheduler.prk_timesteps = np.array([])\n",
    "\n",
    "        traced_unet = load_traced_unet(\n",
    "            MODEL,\n",
    "            subfolder=\"unet_traced\",\n",
    "            filename=\"unet_traced.pt\",\n",
    "            dtype=torch.float32,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        if traced_unet is not None:\n",
    "            print(\"Loaded Traced UNet\")\n",
    "            self.unet = traced_unet\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_images(self, images):\n",
    "        return self.vae.encode(images).latent_dist.sample() * 0.18215\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode_latents(self, latents):\n",
    "        return self.vae.decode(latents / 0.18215).sample\n",
    "\n",
    "    def forward(self, latents, text_embeddings, timesteps):\n",
    "        result = self.unet(\n",
    "            latents,\n",
    "            timesteps,\n",
    "            encoder_hidden_states=text_embeddings\n",
    "        ).sample\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_text_embeddings(self, alpha, text_prompt_start, text_prompt_end):\n",
    "        embed_start = self.frozen_clip.embed_text(text_prompt_start)\n",
    "        embed_end = self.frozen_clip.embed_text(text_prompt_end)\n",
    "        text_embed = embed_start * (1.0 - alpha) + embed_end * alpha\n",
    "        return text_embed, embed_start.dtype\n",
    "\n",
    "    def original_timestep(self, alpha, denoising_a, denoising_b, inference_steps):\n",
    "        strength = (1 - alpha) * denoising_a + alpha * denoising_b\n",
    "\n",
    "        offset = self.scheduler.config.get(\"steps_offset\", 0)\n",
    "        init_timestep = int(inference_steps * strength) + offset\n",
    "        init_timestep = min(init_timestep, inference_steps)\n",
    "\n",
    "        timesteps = self.scheduler.timesteps[-init_timestep]\n",
    "        timesteps = torch.tensor([timesteps], device=self.device)\n",
    "\n",
    "        return timesteps, init_timestep, offset\n",
    "\n",
    "    def partial_diffusion(self, latents, alpha, timesteps, dtype):\n",
    "        noise_a = torch.randn(latents.shape, device=self.device, dtype=dtype)\n",
    "        noise_b = torch.randn(latents.shape, device=self.device, dtype=dtype)\n",
    "        noise = self.slerp(alpha, noise_a, noise_b)\n",
    "        latents = self.scheduler.add_noise(latents, noise, timesteps)\n",
    "        return latents\n",
    "\n",
    "    def get_extra_kwargs(self, eta):\n",
    "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
    "        extra_step_kwargs = {}\n",
    "        if accepts_eta:\n",
    "            extra_step_kwargs[\"eta\"] = eta\n",
    "        return extra_step_kwargs\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def diffuse(\n",
    "            self,\n",
    "            init_image,\n",
    "            text_prompt_start,\n",
    "            text_prompt_end,\n",
    "            inference_steps=50,\n",
    "            denoising_a=0.75,\n",
    "            denoising_b=0.75,\n",
    "            guidance_a=7.0,\n",
    "            guidance_b=7.0,\n",
    "            alpha=0.75,\n",
    "            eta=0.00\n",
    "    ):\n",
    "        self.unet.eval()\n",
    "        self.scheduler.set_timesteps(inference_steps)\n",
    "\n",
    "        # Guidance for later\n",
    "        guidance_scale = guidance_a * (1.0 - alpha) + guidance_b * alpha\n",
    "\n",
    "        text_embed, latents_dtype = self.get_text_embeddings(alpha, text_prompt_start, text_prompt_end)\n",
    "\n",
    "        image_torch = self.preprocess_image(init_image).to(device=self.device, dtype=latents_dtype)\n",
    "        init_latents = self.encode_images(image_torch)\n",
    "\n",
    "        # Partial diffusion\n",
    "        timesteps, init_timestep, offset = self.original_timestep(alpha, denoising_a, denoising_b, inference_steps)\n",
    "        init_latents = self.partial_diffusion(init_latents, alpha, timesteps, dtype=latents_dtype)\n",
    "\n",
    "        extra_step_kwargs = self.get_extra_kwargs(eta)\n",
    "        t_start = max(inference_steps - init_timestep + offset, 0)\n",
    "        timesteps = self.scheduler.timesteps[t_start:].to(self.device)\n",
    "\n",
    "        latents = init_latents\n",
    "        for t in tqdm(timesteps, total=len(timesteps)):\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                pred_noise = self.forward(latents, text_embed, t)\n",
    "                latents = self.scheduler.step(pred_noise, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "        decoded_image = self.decode_latents(latents)\n",
    "        image = (decoded_image / 2 + 0.5).clamp(0, 1).cpu().permute(0, 2, 3, 1).squeeze().numpy()\n",
    "        image = self.numpy_to_pil(image)[0]\n",
    "\n",
    "        return image\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_image(image: Image.Image) -> torch.Tensor:\n",
    "        w, h = image.size\n",
    "        w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
    "        image = image.resize((w, h), resample=Image.LANCZOS)\n",
    "\n",
    "        image_np = np.array(image).astype(np.float32) / 255.0\n",
    "        image_np = image_np[None].transpose(0, 3, 1, 2)\n",
    "\n",
    "        image_torch = torch.from_numpy(image_np)\n",
    "\n",
    "        return 2.0 * image_torch - 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def slerp(t: float, v0: torch.Tensor, v1: torch.Tensor, dot_threshold: float = 0.9995) -> torch.Tensor:\n",
    "        if not isinstance(v0, np.ndarray):\n",
    "            inputs_are_torch = True\n",
    "            input_device = v0.device\n",
    "            v0 = v0.cpu().numpy()\n",
    "            v1 = v1.cpu().numpy()\n",
    "\n",
    "        dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))\n",
    "        if np.abs(dot) > dot_threshold:\n",
    "            v2 = (1 - t) * v0 + t * v1\n",
    "        else:\n",
    "            theta_0 = np.arccos(dot)\n",
    "            sin_theta_0 = np.sin(theta_0)\n",
    "            theta_t = theta_0 * t\n",
    "            sin_theta_t = np.sin(theta_t)\n",
    "            s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
    "            s1 = sin_theta_t / sin_theta_0\n",
    "            v2 = s0 * v0 + s1 * v1\n",
    "\n",
    "        if inputs_are_torch:\n",
    "            v2 = torch.from_numpy(v2).to(input_device)\n",
    "\n",
    "        return v2\n",
    "\n",
    "    @staticmethod\n",
    "    def numpy_to_pil(images):\n",
    "        if images.ndim == 3:\n",
    "            images = images[None, ...]\n",
    "        images = (images * 255).round().astype(\"uint8\")\n",
    "        if images.shape[-1] == 1:\n",
    "            pil_images = [Image.fromarray(image.squeeze(), mode=\"L\") for image in images]\n",
    "        else:\n",
    "            pil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "        return pil_images"
   ],
   "id": "554cc8f02149c6b6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T17:53:56.275113Z",
     "start_time": "2025-01-04T17:53:45.051212Z"
    }
   },
   "cell_type": "code",
   "source": "model = SoundStyleTransferModel()",
   "id": "dac3e2b0528fe20c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch riffusion/riffusion-model-v1: riffusion/riffusion-model-v1 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch riffusion/riffusion-model-v1: riffusion/riffusion-model-v1 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Traced UNet\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T17:54:04.032037Z",
     "start_time": "2025-01-04T17:53:56.291561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image = Image.open(\"in.png\")\n",
    "prompt_start = \"Church bells on sunday\"\n",
    "prompt_end = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8K\"\n",
    "\n",
    "image = model.diffuse(image, prompt_start, prompt_end)\n",
    "\n",
    "image.save(\"out.png\")"
   ],
   "id": "d900bfc2c63eef0f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:07<00:00,  5.28it/s]\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
